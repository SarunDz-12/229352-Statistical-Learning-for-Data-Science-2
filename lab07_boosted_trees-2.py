# -*- coding: utf-8 -*-
"""Lab07_Boosted_trees.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uwublIxohi32UJcEGowCfnxl-Crs2szP

### Statistical Learning for Data Science 2 (229352)
#### Instructor: Donlapark Ponnoprat

#### [Course website](https://donlapark.pages.dev/229352/)

## Lab #6

## Boosted tree models on a simulated dataset

- [AdaBoostClassifier documentation](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html#sklearn-ensemble-adaboostclassifier)
- [XGBClassifier documentation](https://xgboost.readthedocs.io/en/stable/python/python_api.html#xgboost.XGBClassifier)
- [LGBMClassifier documentation](https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMClassifier.html#lightgbm-lgbmclassifier)
- [GridSeachCV documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)


- [Data](https://github.com/donlapark/ds352-labs/raw/main/Lab06-data.zip)

Perform GridSearchCV of the following three models on the provided training set (`X_train.csv` and `y_train.csv`)

1. Evaluate these models on the test set (`X_test.csv` and `y_test.csv`). **Keep searching (using cross-validation) until you find the model that achieves > 0.83 out-of-fold accuracy (use `GridSeachCV.best_score_` to obtain the out-of-fold accuracy)**

2. Report the test accuracy of your best model.

3. For each model, plot the feature importances

For `AdaBoostClassifier`, feature importances can be obtained by calling the `feature_importances_` attribute after fitting the model.

For `XGBClassifier` and `LGBMClassifier`, feature importances can be obtained using the library’s `plot_importance` function. Here is a minimal example in XGBoost:
"""

from sklearn import datasets


iris = datasets.load_iris()
X = iris.data
y = iris.target

from sklearn.ensemble import AdaBoostClassifier


ab = AdaBoostClassifier()
ab.fit(X, y)
ab.feature_importances_

from xgboost import XGBClassifier, plot_importance


model = XGBClassifier()
model.fit(X, y)
plot_importance(model);

X

from xgboost import plot_tree

plot_tree(model, num_trees=4);

import pandas as pd

X_train = pd.read_csv('X_train.csv', header=None)


X_train

{
  "n_estimators": [100, 200],
  "learning_rate": [0.1, 1.0]
}

{
  "n_estimators": [100],
  "max_depth": [3, 5],
  "learning_rate": [0.1]
}

{
  "n_estimators": [100, 200],
  "learning_rate": [0.05, 0.1]
}

import pandas as pd
from sklearn.model_selection import GridSearchCV
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.metrics import accuracy_score
from sklearn.ensemble import AdaBoostClassifier
from xgboost import XGBClassifier, plot_importance
from lightgbm import LGBMClassifier

y_train = pd.read_csv("/content/y_train.csv").values.ravel()
X_train = pd.read_csv("X_train.csv")

print(X_train.shape)
print(y_train.shape)

"""# **AdaBoost**"""

import pandas as pd
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import AdaBoostClassifier

X_train = pd.read_csv("X_train.csv")
y_train = pd.read_csv("y_train.csv").values.ravel()

model = AdaBoostClassifier(random_state=42)

param_grid = {
    "n_estimators": [100, 200],
    "learning_rate": [0.1, 1.0]
}

gs = GridSearchCV(model, param_grid, cv=5, scoring="accuracy")
gs.fit(X_train, y_train)

print(gs.best_score_)

esti = AdaBoostClassifier()

esti.get_params()

from sklearn.model_selection import GridSearchCV
params = { 'n_estimators': [10,25,50,100,150,200],'learning_rate': [0.5,1.0,2.0,3.0,4.0,5.0]}
gridAda = GridSearchCV(esti,params,scoring='accuracy')

gridAda.fit(X_train,y_train)

gridAda.best_score_

gridAda.best_params_

gridAda.best_score_

from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.ensemble import AdaBoostClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import classification_report

pipe = Pipeline([
    ("imputer", SimpleImputer(strategy="mean")),
    ("model", AdaBoostClassifier(random_state=42))
])

param_grid = {
    "model__n_estimators": [100, 200],
    "model__learning_rate": [0.1, 1.0]
}

gridAda = GridSearchCV(
    pipe,
    param_grid,
    cv=5,
    scoring="accuracy"
)

gridAda.fit(X_train, y_train)

best_model = gridAda.best_estimator_

# สำคัญ: จัดคอลัมน์
X_test = X_test.reindex(columns=X_train.columns)

y_pred = best_model.predict(X_test)

print(classification_report(y_test, y_pred))

from sklearn.ensemble import AdaBoostClassifier

ada = AdaBoostClassifier(
    learning_rate=1.0,
    n_estimators=50,
    random_state=42
)

ada.fit(X_train, y_train)

importances = ada.feature_importances_
print(importances)

"""# **XGBoots**"""

xg = XGBClassifier()
params = {'n_estimators':[25,50,100,150,200], 'max_depth':[1,2,3,4,5,6], 'learning_rate':[1,2,3,4]}
gridxg= GridSearchCV(xg, params, scoring='accuracy')

gridxg.fit(X_train, y_train)

gridxg.best_params_

gridxg.best_score_

y_pred_xg = gridxg.predict(X_test)
print(classification_report(y_test, y_pred_xg))

from xgboost import plot_importance
xgplot = XGBClassifier(learning_rate= 1, max_depth= 3, n_estimators= 50)
xgplot.fit(X_train, y_train)
plot_importance(xgplot);

"""# **LGBM**"""

from lightgbm import LGBMClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score

# สร้างโมเดล
lgbm = LGBMClassifier(random_state=42)

# กำหนดพารามิเตอร์
param_grid = {
    "n_estimators": [100, 200, 300],
    "learning_rate": [0.05, 0.1, 0.2],
    "max_depth": [-1, 5, 10],
    "num_leaves": [31, 50, 100]
}

# GridSearchCV
grid_lgbm = GridSearchCV(
    estimator=lgbm,
    param_grid=param_grid,
    cv=5,
    scoring="accuracy",
    n_jobs=-1
)

# train
grid_lgbm.fit(X_train, y_train)

grid_lgbm.best_score_

best_lgbm = grid_lgbm.best_estimator_

X_test = X_test.reindex(columns=X_train.columns)

y_pred = best_lgbm.predict(X_test)
test_acc = accuracy_score(y_test, y_pred)

print("Test accuracy:", test_acc)

import matplotlib.pyplot as plt
from lightgbm import plot_importance

plt.figure()
plot_importance(best_lgbm, max_num_features=10)
plt.title("Feature Importances (LGBM)")
plt.show()